<!DOCTYPE html>
<html lang = "en">

<head>

<title>@Hapslab | Data anything</title>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<meta property="og:locale" content="en_US" />
<meta property="og:image" content="hapslab-logo.jpg">
<meta property="og:site_name" content="hapslab">
<meta property="og:title" content="Have fun @hapslab" />
<meta property="og:url" content="" />
<meta property="og:description" content="Data Products, Services, Solutions & Architecture Of Any Scale & Latency. Developing & Productionizing Products With Big Data, Data Analytics, Data Science, Artificial Intelligence. Data Compute Via ETL, Streaming Or Realtime. Cloud Presence. Point In Time Data Delivery Via Microservices. Security & Page Portals">


<link   rel  = "stylesheet"      href = "hapslabLib.css"> 
<script type = "text/javascript" src  = "hapslabLib.js"></script>

<style>

html 
{
    background-color: #F6F6F6;     /* #E9E9E9; #DCDCDC #FBFBFB; #FCFBFB; #F6F5F5; PAGE color background color */
    width: 99.15vw;
    color: #01014c;                /* #020273 #01015f; #01014c;, #555555 perfect grey, #13133a; in BLOG */
    font-family: fantasy;
    font-weight: normal;
    font-style: normal;
    text-align: left;
    vertical-align: text-bottom;
}
</style>

<script src="hapslabHeaderFooter.js"> </script>
<script src="hapslabPictureBlogs.js">    </script>

<!--
        <script src="hapslabContacts.js"></script>
changed to hapslabHeaderFooter.js from hapslabHeaderFooter-11.js
hapslabHeaderFooterContacts.js
--> 

</head>

<body onload="hapslabWrapper(); ">

<!------------------------------------------- PAGE HEADER --------------------------------------------->
<!------------------------------------------- PAGE HEADER --------------------------------------------->
<!------------------------------------------- PAGE HEADER --------------------------------------------->
<!------------------------------------------- PAGE HEADER --------------------------------------------->

<style>     

table { position: relative; margin: 9.5vh 3vw 1.5vh 3vw; }

table, th, td { font-family:verdana; border: 1px solid black; text-align:left; width:auto; }      

.table-header  { font-size:90%; font-weight:bold; }       

.table-rows    { font-size:60%; font-weight:normal; }     

a.a-tag { color:#000033; text-decoration:none; border-bottom:.1px solid #000033; }     

a.a-algoname { color:#660000; text-decoration:none; border-bottom:.1px solid #660000; font-size:150%; }     

a.a-algodesc { color:#660000; font-size:120%; }     

#h1-header-container
{  
   position: relative;
   background-color: white;      /* background color for the container #f3e7ff #8000ff */
   height: auto;                 /* Dimension trends-box-container */
   width: auto;                  /* Corrects Trends Column sheet width. 105% includes 1 more colm */
   font-family: Arial;

   float: center;
   text-align: center;

   margin-left: 3vw;
   margin-right: 3vw;
   margin-top: 20vh;             /* GAP OUTSIDE below trends-columns-container */
   margin-bottom: 20vh;          /* GAP OUTSIDE below trends-columns-container */
   padding: 10vh 2vw 15vh 2vw;
// border: 1px solid red;
}

.h1-header  
{
   position: relative;
   font-size: 2.5vw; 
   font-weight: bold;
   text-align: center;
//   margin-top: 5vh;             /* GAP OUTSIDE below trends-columns-container */
   margin-bottom: 10vh;          /* GAP OUTSIDE below trends-columns-container */

}

.h1-header-img         /* + or - SIZE of image */
{
   float: center;
   margin-left: auto;      /* Centers image */
   margin-right: auto;     /* Centers image */
   width: 75%;
   height: 65%;
  // border: 1px solid silver;
   margin-bottom: 9vh;
}
  
.h1-header-details
{
   position: relative;
   float: center;
   color: #414141;    /* #3E3E3E light grey color */ 
   font-size: .6vw;
   font-weight:normal;
   margin-top: .5vh;
}

</style>     


<div id="org-ddm-id" class="org-ddm" >

     <div class="menu-box-header-container">
     </div>


<!------------------------------------------- PAGE CENTER - START --------------------------------------------->
<!------------------------------------------- PAGE CENTER - START --------------------------------------------->
<!------------------------------------------- PAGE CENTER - START --------------------------------------------->
<!------------------------------------------- PAGE CENTER - START --------------------------------------------->


     <div id="h1-header-container">

          <div class="h1-header">Data Science, DL, ML
               <div class="h1-header-details">
                    <span>by 
                          <b>Ritendra Srivastav</b> on 
                          <b>6 January 2020</b> in 
                          <b><a class="h1-header-details" href="www.hapslab.com/category.html" data-lbl="category" title="Category">Category: DLML</a></b>, 
                          <b><a class="h1-header-details" href="www.hapslab.com/subcategory.html" data-lbl="sub-category" title="Sub-category">Sub-category: Algorithms</a></b>
                    </span>

<!--
<br><b>PS: Originally published <a class="trends-blogs-details" href="https://hapslab.blogspot.com" data-lbl="Hapslab blog" title="Hapslab blog">Hapslab blog</a></b>

     <style>     table { margin:0 }     table, th, td { font-family:verdana; border: 1px solid black; text-align:left; width:1vw; }      .table-header  { font-size:1vw; font-weight:bold; }       .table-rows    { font-size:.75vw; font-weight:normal; }     a.a-tag { color:#000033; text-decoration:none; border-bottom:.1vw solid #000033; }     a.a-algoname { color:#660000; text-decoration:none; border-bottom:.1vw solid #660000; font-size:1.5vw; }     a.a-algodesc { color:#660000; font-size:1.2vw; }     </style>     

<h1>Introduction</h1>
<div class="para" id="intro"><pre><ul>

</ul></pre></div>

-->

               </div>
          </div>

     <img class="h1-header-img" src="hl-ai-landscape.jpg" alt="Data Science, DL, ML" title="Data Science, DL, ML">


<div class="a-tag-index"> <a style="font-size: 1vw;" href="#intro">Introduction</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#linreg">Linear Regression</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#multireg">Multiple Linear Regression</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#logireg">Logistic Regression</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#svm">SVM</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#knn">k-Nearest Neighbour</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#naibay">Naive Bayes</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#dectre">Decision Trees</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#ranfor">Random Forest</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#rfm">Response, Frequency, Monetory</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#aprior">Apriori</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#adalin">Adaline</a> </div>
<div class="a-tag-index"> <a style="font-size: 1vw;" href="#dimred">Dimensionality Reduction</a> </div>




<table cellpadding="5"> 
<tr><th class="table-header" style="width: 1%; ">Category/ Sub Category</th>
<th class="table-header" style="width: 15%;">Use case</th>
<th class="table-header" style="width: 10%;">Algorithm or Model</th>
<th class="table-header" style="width: 40%;">Description</th>
<th class="table-header" style="width: 30%;">Practices, Techniques or Ratios</th>
<th class="table-header" style="width: 30%;">Hypertuning (tunes estimators)</th>
<th class="table-header" style="width: 30%;">Packages</th>
<th class="table-header" style="width: 5%;"><span style="color: green;">Pros</span> / <span style="color: red;">Cons</span></th>


</tr>

<tr><td id="linreg" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised<a> learning / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;">Predicting a <b>continuous</b> variable (y) over (x1) <b>single variable</b>. y is dependent or target variable and x1 is independent or feature variable. This is called <b>Simple Linear Regression</b></td>
<td class="table-rows" style="width: 10%;"><div id="linreg"></div><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised<a> learning / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a><br><a class="a-algoname" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Linear Regression">Linear Regression</a> <b class="a-algoname">Predicts a category value that is discrete, finite with no ordering.</b>.</td>
<td class="table-rows" style="width: 40%;">Here y = w0 + w1.x1. (w0 is bias, w1 is weight). Bias (w0) is where our <b>straight line</b> intercepts y-axis. We optimize w0 and w1 to predict y. Example: Predicting house prices, stock prices, toys sales etc. <br>For house price prediction data - RadioAd, TVAd, News, Sales. Simple linear regression equation will be Sales (y) = w1 . RadioAd + w0. <b>Uses single variable (x1).</b><br><b>Properties:</b> 1. Uses a straight line. 2. Relationship between dependent and independent variables. 3. Independent variables could be correlated, specially in Multiple Linear Regression or Multiple Regression (<b>MLR</b>)<br>It is based on <a class="a-tag" href="https://en.wikipedia.org/wiki/Least_squares" target="_blank" title="Least square estimation">Least square estimation</a> or Squares Sum of Errors (SSE).</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="SSE">SSE</a> should be minimum for the accurate predicted value. <a class="a-tag" href="https://en.wikipedia.org/wiki/Errors_and_residuals" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Regression Error and Regression Residual">Regression Error and Regression Residual</a> provides regression loss and regression accuracy Helps in estimating bias (w0),   <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="R2 Score">R2 Score</a> provides regression score.</td>
<td class="table-rows" style="width: 30%;">Best prediction will have: SSE=0.0, R2 score=1.0.<br><a class="a-tag" href="https://scikit-learn.org/stable/modules/learning_curve.html" target="_blank" title="Learning">Learning</a> must reduce <a class="a-tag" href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_bias_variance.html" target="_blank" title="bias and variance">bias and variance</a> normalizing <a class="a-tag" href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Overfitting">Overfitting</a> and <b>Underfitting</b>.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.LinearRegression">sklearn.linear_model.LinearRegression</a>, <br> <a class="a-tag" href="https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.linalg.lstsq.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="numpy.linalg.lstsq">numpy.linalg.lstsq<a> (OLS), <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.metrics.mean_squared_error">sklearn.metrics.mean_squared_error</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.metrics.r2_score">sklearn.metrics.r2_score</a></td>
<td class="table-rows" style="width: 5%;"><span style="color: orange;">Minimize of <a class="a-tag" href="https://en.wikipedia.org/wiki/Residual_sum_of_squares" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Sum of Squares Error">Sum of Squares Error</a> (SSE) makes linear regression very sensitive to  <a class="a-tag" href="https://en.wikipedia.org/wiki/Anomaly_detection" target="_blank" title="anomaly detection">anomaly detection</a> needing re-trainings.</span></td>


</tr>

<tr><td id="multireg" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;"><b>Predicting house prices</b> (y) a dependent or target variable <b>continuous</b> variable from (x1, x2, .. xn) <b>multiple independent or feature variables</b>. Predict house price (y) from these features - RadioAd, TVAd, News, Sales. Simple linear regression equation will be Sales (y) = w0 + w1.RadioAd + w2.TVAd + w3.News + E <b>Using three variables</b></td>
<td class="table-rows" style="width: 10%;"><a class="a-algoname" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Multiple Linear Regression or Multiple Regression">Multiple Linear Regression or Multiple Regression</a> (MLR). <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Linear Regression">Linear Regression</a> <b class="a-algoname">Predicts a category value that is discrete, finite with no ordering.</b>.</td>
<td class="table-rows" style="width: 40%;">Here y = w0 + w1x1 + .. wnxn + E. w1, w2 .. wn, are weights and x1, x2, .. xn, are features. Bias (w0) is where our <b>straight line</b> intercepts y-axis and E model error is the difference between actual outcome and the predicted outcome. Predicting a target variable (y) using multiple feature variables w0 .. wn is <b>Multiple Linear Regression (MLR) or Multiple Regression</b>. <br>Example: Predicting house prices, stock prices, toys sales etc. <b>Uses multiple variables (x1, x2 ....xn).</b><br>It is based on <a class="a-tag" href="https://en.wikipedia.org/wiki/Least_squares" target="_blank" title="Least square estimation">Least square estimation</a> or Squares Sum of Errors (SSE).<br><b>Properties:</b> Independent variables must be correlated in Multiple Linear Regression or Multiple Regression (<b>MLR</b>).</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Multiple Linear Regression or Multiple Regression">Multiple Linear Regression or Multiple Regression</a> using <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html" target="_blank" title="sklearn.linear_model.RANSACRegressor">sklearn.linear_model.RANSACRegressor</a> will be robust ignoring <b>outliers</b> and selecting <b>inliers</b> data. <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" title="Train test split">Train test split</a> will split the overall dataset into appropriate ratio requested. <br><a class="a-tag" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="OLS">OLS</a> is fast at learning time and prediction time hence should be used first for regression problems. <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="MSE">MSE</a> provides regression loss and compares different regression models, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Lasso (L1)">Lasso (L1)</a> & <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Ridge (L2)">Ridge (L2)</a> handles overfitting or underfitting via regularization. L2 improves <a class="a-tag" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="OLS">OLS</a>. <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="MSE">MSE</a> provides regression loss, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="R2 Score">R2 Score</a> provides regression score.</td>
<td class="table-rows" style="width: 30%;"><b>Normalize</b>: normalize=True (default), to standardize use normalize=False with <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html" target="_blank" title="sklearn.preprocessing.StandardScaler">sklearn.preprocessing.StandardScaler</a>. <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/cross_validation.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Cross Validation">Cross Validation</a> strategies can help in model selection: <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" target="_blank" title="cross_val_score">cross_val_score</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold" target="_blank" title="GroupKFold">GroupKFold</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold" target="_blank" title="KFold">KFold</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut" target="_blank" title="LeaveOneGroupOut">LeaveOneGroupOut</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneOut.html#sklearn.model_selection.LeaveOneOut" target="_blank" title="LeaveOneOut">LeaveOneOut</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePOut.html#sklearn.model_selection.LeavePOut" target="_blank" title="LeavePOut">LeavePOut</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html#sklearn.model_selection.ShuffleSplit" target="_blank" title="ShuffleSplit">ShuffleSplit</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeavePGroupsOut.html#sklearn.model_selection.LeavePGroupsOut" target="_blank" title="LeavePGroupsOut">LeavePGroupsOut</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedKFold.html#sklearn.model_selection.RepeatedKFold" target="_blank" title="RepeatedKFold">RepeatedKFold</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold" target="_blank" title="RepeatedStratifiedKFold">RepeatedStratifiedKFold</a>,  <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold" target="_blank" title="StratifiedKFold">StratifiedKFold</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html#sklearn.model_selection.StratifiedShuffleSplit" target="_blank" title="StratifiedShuffleSplit">StratifiedShuffleSplit</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html#sklearn.model_selection.TimeSeriesSplit" target="_blank" title="TimeSeriesSplit">TimeSeriesSplit</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html#sklearn.model_selection.train_test_split" target="_blank" title="train_test_split">train_test_split</a>.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html" target="_blank" title="sklearn.linear_model.RANSACRegressor">sklearn.linear_model.RANSACRegressor</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html" target="_blank" title="sklearn.model_selection.train_test_split">sklearn.model_selection.train_test_split</a>, <br><a class="a-tag" href="https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.lstsq.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="OLS">OLS</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.score" target="_blank" title="R<sup>2</sup> Score">R<sup>2</sup> Score</a>
, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.model_selection.cross_validate">sklearn.model_selection.cross_validate</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.model_selection.cross_val_score">sklearn.model_selection.cross_val_score</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.model_selection.KFold">sklearn.model_selection.KFold</a> Cross Validation, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.model_selection.GridSearch">sklearn.model_selection.GridSearch</a>. Both <a class="a-tag" href=https://scikit-learn.org/stable/modules/cross_validation.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Cross Validation">Cross Validation</a> and <a class="a-tag" href="https://scikit-learn.org/stable/modules/grid_search.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Grid Search">Grid Search</a> packages have many variations per use case. <a class="a-tag" href="https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Cross-validation generators">Cross-validation generators</a> and others <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" target="_blank" title="sklearn.linear_model.Ridge">sklearn.linear_model.Ridge</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html" target="_blank" title="sklearn.linear_model.RidgeCV">sklearn.linear_model.RidgeCV</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html" target="_blank" title="sklearn.linear_model.Lasso">sklearn.linear_model.Lasso</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html" target="_blank" title="sklearn.linear_model.LassoCV">sklearn.linear_model.LassoCV</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html" target="_blank" title="sklearn.linear_model.ElasticNet">sklearn.linear_model.ElasticNet</a></td>
<td class="table-rows" style="width: 5%;"></td>


</tr>

<tr><td id="logireg" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;">If customer buys home <b>Yes</b> else <b>No</b>. Key features here: <b>Price</b>, house <b>Rating</b>, <b>Reviews</b>. If we use <b>Reviews</b> here we will be using NLP algorithms and Grid Search OR we can use Precision and Recall. If we use Price we can use Gradient Descent algorithm. Example: House will be sold for X price or not; Deciding if email is spam or not; Online transaction is fraudulent or not etc.</td>
<td class="table-rows" style="width: 10%;"><a class="a-algoname" href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Logistic Regression">Logistic Regression</a> <b class="a-algoname">Predicts a numeric value that is continuous and infinite with ordering</b>. It is based on <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance.fit" target="_blank" title="Maximum Likelihood Estimation">Maximum Likelihood Estimation</a> (MLE). <a class="a-tag" href="https://en.wikipedia.org/wiki/Logistic_function" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Logistic function">Logistic function<a> or sigmoid function, helps to achieve this.</td>
<td class="table-rows" style="width: 40%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Logistic Regression">Logistic Regression</a> is a linear model for classification rather than regression. Logistic regression can be <a class="a-tag" href="https://en.wikipedia.org/wiki/binary_regression" target="_blank" title="binary">binary</a> have an S sort of curve due to binary values of 1 and 0 (a special case of <a class="a-tag" href="https://en.wikipedia.org/wiki/binomial_regression" target="_blank" title="binomial regression">binomial regression</a>), <a class="a-tag" href="https://en.wikipedia.org/wiki/ordinal_regression" target="_blank" title="ordinal">ordinal</a> performed via <a class="a-tag" href="https://en.wikipedia.org/wiki/generalized_linear_model" target="_blank" title="generalized linear model">generalized linear model</a> (GLM), <a class="a-tag" href="https://en.wikipedia.org/wiki/nonparametric_regression" target="_blank" title="nonparametric ">nonparametric</a> based on <a class="a-tag" href="https://en.wikipedia.org/wiki/parametric_model" target="_blank" title="parametric model">parametric model</a>. Most common binary regression models are <a class="a-tag" href="https://en.wikipedia.org/wiki/logit_model" target="_blank" title="logit model">logit model</a> (<a class="a-tag" href="https://en.wikipedia.org/wiki/logistic_regression" target="_blank" title="logistic regression">logistic regression</a>) and <a class="a-tag" href="https://en.wikipedia.org/wiki/probit_model" target="_blank" title="probit model">probit model</a> (<a class="a-tag" href="https://en.wikipedia.org/wiki/probit_regression" target="_blank" title="probit regression">probit regression</a>).<br><a class="a-tag" href="https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Logistic Regression">Logistic Regression<a> is based on <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html#sklearn.covariance.EmpiricalCovariance.fit" target="_blank" title="Maximum Likelihood Estimation">Maximum Likelihood Estimation</a> (MLE) and uses <a class="a-tag" href="https://en.wikipedia.org/wiki/Logistic_function" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Logistic function">Logistic function<a> or sigmoid function.<br><b>Properties:</b> 1. Uses a curve. 2. No relationship between dependent and independent variables. 3. No correlation between independent variables.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Lasso (L1)">Lasso (L1)</a> shrinks all dependent variables except one to near zero, & <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Ridge (L2)">Ridge (L2)</a> spreads the shrinkage for all interdependent variables making them equally influential. L1 & L2 both are penalties, they handle <a class="a-tag" href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Overfitting">Overfitting</a> and <b>Underfitting</b> via <b>Regularization (applied by default)</b>, it can handle both dense and sparse input. L2 improves <a class="a-tag" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="OLS">OLS</a>. <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html" target="_blank" title="ElasticNet">ElasticNet (Combined L1& L2)</a> and one-vs-rest (OvR) are other penalties. Here <a class="a-tag" href="https://en.wikipedia.org/wiki/Confusion_matrix" target="_blank" title="Confusion matrix">Confusion matrix</a> provides <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification_rule" target="_blank" title="classification rules">classification rules</a>. <br><a class="a-tag" href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank" title="Precision and recall">Precision and recall</a> helps to find huge number of accurate answers or true positives and <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html" target="_blank" title="ROC AUC Score">ROC AUC Score</a> is best for classifier performance & comparing different classifiers. For sparse features where  <a class="a-tag" href="https://en.wikipedia.org/wiki/Precision_and_recall" target="_blank" title="Precision and recall">Precision and recall</a> wont work accurately we can use  <a class="a-tag" href="https://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_early_stopping.html" target="_blank" title="Stochastic Gradient Descent">Stochastic Gradient Descent</a><br>We should try to find true positives (only yes or no) <br><a class="a-tag" href="https://en.wikipedia.org/wiki/Chi-squared_test" target="_blank" title="Chi-square test">Chi-square test</a>  determines if variables are independent.</td>
<td class="table-rows" style="width: 30%;"><b>penalty</b> (Can have l2 (default); l1; elastinet; or none), <b>l1_ratio</b> (l1_ratio=0 is same as penalty='l2'; l1_ratio=1 is same as penalty='l1'; if penalty='elasticnet' 0 <= l1_ratio <= 1), <b>multi_class</b> (If 'ovr' then a binary problem), <b>solver</b> (For small datasets, 'liblinear' (for OvR only); For huge datasets 'sag' and 'saga';) (For multiclass problems: 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss). densify() and sparsify() can help with dense and sparse data.<br><a class="a-tag" href="https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Cross-validation">Cross-validation</a> estimators are preferred over <a class="a-tag" href="https://scikit-learn.org/stable/modules/grid_search.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Grid Search">Grid Search</a> due to speed improvements.  Tune Precision/Class good P & Recall/Real R P=(TP/TP+FP) & R=(TP/TP+FN), more Are Under Curve AUC.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.LogisticRegression">sklearn.linear_model.LogisticRegression</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.LogisticRegressionCV">sklearn.linear_model.LogisticRegressionCV</a> (built-in cross validation), <br><a class="a-tag" href="https://scikit-learn.org/stable/tutorial/statistical_inference/model_selection.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Cross-validation generators">Cross-validation generators</a> and others <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html" target="_blank" title="sklearn.linear_model.Ridge">sklearn.linear_model.Ridge</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html" target="_blank" title="sklearn.linear_model.Lasso">sklearn.linear_model.Lasso</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html" target="_blank" title="sklearn.model_selection.cross_val_score">sklearn.model_selection.cross_val_score</a>, <br>
<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html" target="_blank" title="sklearn.metrics.precision_recall_curve">sklearn.metrics.precision_recall_curve</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.SGDRegressor">sklearn.linear_model.SGDRegressor</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.SGDClassifier">sklearn.linear_model.SGDClassifier</a>
<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html" target="_blank" title="sklearn.metrics.roc_auc_score">sklearn.metrics.roc_auc_score</a></td>
<td class="table-rows" style="width: 5%;"><span style="color: red;">Due to repeatitive iterations until the model finds MLE its compute intensive.</span></td>


</tr>

<tr><td id="svm" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;">Home loan Approved or Not from a bank. Features used: <b>Job, Type, Income, Dependents, Loan amount, Credit history</b>.</td>
<td class="table-rows" style="width: 10%;"><a class="a-algoname" href="https://ttps://scikit-learn.org/stable/modules/svm.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="SVM">SVM</a> has implementations of <a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="supervised learning">supervised learning<a> methods in <b>Support Vector</b> <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#classification" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Classification">Classification</a> (<b>SVC</b>), <b>Support Vector</b> <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Regression">Regression</a> <b>SVR</b> and <a class="a-tag" href="https://en.wikipedia.org/wiki/Unsupervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="unsupervised learning">unsupervised learning<a> based <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="outlier detection">outlier detection</a> model.</td>
<td class="table-rows" style="width: 40%;"><a class="a-tag" href="https://ttps://scikit-learn.org/stable/modules/svm.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="SVM">SVM</a> has implementations of <a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="supervised learning">supervised learning<a> methods in <b>Support Vector</b> <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#classification" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Classification">Classification</a> (<b>SVC</b>) (<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.SVC">SVC</a>, 
<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.NuSVC">NuSVC</a> and 
<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVC">LinearSVC</a> can perform <a class="a-tag" href=" https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification-format" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="multiclass multilabel multioutput">multiclass multilabel classification and multioutput regression</a>), <b>Support Vector</b> <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Regression">Regression</a> <b>SVR</b> (<a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.SVR">SVR</a>, <a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.NuSVR">NuSVR</a> and 
<a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVR">LinearSVR</a>) and <a class="a-tag" href="https://en.wikipedia.org/wiki/Unsupervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="unsupervised learning">unsupervised learning<a> based <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="outlier detection">outlier detection</a> model <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.OneClassSVM">OneClassSVM</a>. <a class="a-tag" href="https://ttps://scikit-learn.org/stable/modules/svm.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="SVM">SVM</a> is effective in high dimensional spaces. Even if number of dimensions is > number of features. Here we plot all data items as a point in n-dimensional space (where n is number of features). <a class="a-tag" href="https://ttps://scikit-learn.org/stable/modules/svm.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="SVM">SVM</a> helps us to classify hyper-planes where set of features will belong. SVM is used for <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#svm-regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer"  title="Regression">Regression</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#svm-classification" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Classification">Classification</a>, and <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Outlier Detection">Outlier Detection</a>, more <a class="a-tag" href="https://scikit-learn.org/stable/modules/outlier_detection.html#outlier-detection" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="here">here</a>. <a class="a-tag" href="https://ttps://scikit-learn.org/stable/modules/svm.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="SVM">SVM</a> support both dense data via <i>numpy.ndarray</i> and sparse data via <i>scipy.sparse</i>. <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.SVC">SVC</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.NuSVC">NuSVC</a>, <a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html#sklearn.svm.SVR" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.SVR">SVR</a>, and <a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.NuSVR">NuSVR</a>. All of these use <a class="a-tag" href="https://www.csie.ntu.edu.tw/~cjlin/libsvm/" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="LIBSVM">LIBSVM</a> (<b>Library for <a class="a-tag" href="https://ttps://scikit-learn.org/stable/modules/svm.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="SVM">SVM</a></b>). Only <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVC">LinearSVC</a>, <a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVR">LinearSVR</a>) and <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.SGDRegressor">SGDRegressor</a> implement <a class="a-tag" href="https://www.csie.ntu.edu.tw/~cjlin/liblinear/" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="LIBLINEAR">LIBLINEAR</a> (<b>Library for Large Linear Classification</b>) uses <a class="a-tag" href="https://en.wikipedia.org/wiki/Linear_classifier" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="linear classifier">linear classifier</a>.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#kernel-functions" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Kernel functions">Kernel functions</a> transforms  low dimensional input space to a higher dimensional space thus seperating them, adding more accuracy. Kernel methods has Linear method, kernel=linear (default), <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#using-the-gram-matrix" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Gram Matrix">Gram Matrix</a> (kernel-precomputed), and <a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="RBF Kernel">RBF Kernel</a> (kernel=rbf) Radial Basis Function (RBF), also polynomial, sigmoid can be set. <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVC">LinearSVC</a> (handles <a class="a-tag" href=" https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification-format" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="multiclass multilabel multioutput">multiclass classification</a> via one-vs-the-rest scheme (OvR), multi_class='ovr') and <a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVR">LinearSVR</a> (handles <a class="a-tag" href=" https://scikit-learn.org/stable/modules/multiclass.html#multilabel-classification-format" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="multiclass multilabel multioutput">multioutput regression</a> via one-vs-the-rest scheme (OvR), multioutput='uniform_average') both handle penalties and loss function scaling over huge samples along with dense and sparse input.</td>
<td class="table-rows" style="width: 30%;">With <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.SVC">sklearn.svm.SVC</a> <b>kernel</b>=<a class="a-tag" href="https://scikit-learn.org/stable/modules/svm.html#parameters-of-the-rbf-kernel" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="RBF kernel">rbf</a> set <b>C</b> and <b>gamma</b>. <b>Coeff or  score()</b> internally uses multioutput support.<br><b>kernel</b>: 'linear', 'poly', 'sigmoid', 'rbf'. <b>With kernel=liblinear, many other parameter settings go void</b>. Adjust <b>penalty and loss</b> for <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.SGDRegressor">SGDRegressor</a> and <a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html#sklearn.svm.LinearSVR" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVR">LinearSVR</a> to optimize same cost function, takes <b>less memory, implements various loss functions and regularization regimes.</b>. <b>Coef0</b> is independent for 'poly' (<b>degree</b> too) and 'sigmoid' kernels.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.SVC">sklearn.svm.SVC</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.NuSVC">sklearn.svm.NuSVC</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.SVR">sklearn.svm.SVR</a>, <a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.NuSVR">sklearn.svm.NuSVR</a>. <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVC">sklearn.svm.LinearSVC</a>, <a class=" a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.svm.LinearSVR">sklearn.svm.LinearSVR</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.SGDRegressor">sklearn.linear_model.SGDRegressor</a></td>
<td class="table-rows" style="width: 5%;"><span style="color: green;">Good when number of dimensions is more than number of samples (use kernel functions).</span><br> <span style="color: red;">Huge data has higher training time due to five-fold cross-validation.</span><br><span style="color: red;">Bad for noisy data set i.e. target classes overlapping.</span></td>


</tr>

<tr><td id="knn" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;">Detecting poor answers from groups or classes or types.</td>
<td class="table-rows" style="width: 10%;"><a class="a-algoname" href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm" target="_blank" title="K-Nearest Neighbors">K-Nearest Neighbors</a> (KNN). <a class="a-tag" href="https://scikit-learn.org/stable/modules/neighbors.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="nearest neighbors">Nearest neighbors</a> has <a class="a-tag" href="https://en.wikipedia.org/wiki/Unsupervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="unsupervised">unsupervised<a> models into: <a class="a-tag" href="https://scikit-learn.org/stable/modules/clustering.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="clustering">clustering</a> and <a class="a-tag" href="https://scikit-learn.org/stable/modules/manifold.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="manifold">manifold</a>. <a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="supervised">Supervised<a> models into <a class="a-tag" href="https://://en.wikipedia.org/wiki/Classification" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="classification">classification</a> and <a class="a-tag" href="https://en.wikipedia.org/wiki/Regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="regression">regression</a>.</td>
<td class="table-rows" style="width: 40%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/neighbors.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="nearest neighbors">Nearest neighbors</a> has <a class="a-tag" href="https://en.wikipedia.org/wiki/Unsupervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="unsupervised">Unsupervised<a> learning models: <a class="a-tag" href="https://scikit-learn.org/stable/modules/clustering.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="clustering">clustering</a> models (<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.cluster.DBSCAN">sklearn.cluster.DBSCAN</a>, 
<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.cluster.SpectralClustering">sklearn.cluster.SpectralClustering</a>) and <a class="a-tag" href="https://scikit-learn.org/stable/modules/manifold.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="manifold">manifold</a> models (<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.manifold.TSNE">sklearn.manifold.TSNE</a> and <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.manifold.Isomap">sklearn.manifold.Isomap</a>). <a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="supervised">Supervised<a> learning models have <a class="a-tag" href="https://://en.wikipedia.org/wiki/Classification" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="classification">classification</a> (discrete labeled data), and <a class="a-tag" href="https://en.wikipedia.org/wiki/Regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="regression">regression</a> (continuous labeled data).<br><a class="a-tag" href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Nearest neighbors estimators">Nearest neighbors estimators</a> use functions <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.neighbors.kneighbors_graph">kneighbors_graph</a> and <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.radius_neighbors_graph.html#sklearn.neighbors.radius_neighbors_graph" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.neighbors.radius_neighbors_graph">radius_neighbors_graph</a> generating <b>binary adjacency</b> <a class="a-tag" href="https://scikit-learn.org/stable/glossary.html#term-sparse-graph" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sparse graph">sparse graph</a> (with, mode='connectivity') and <b>distance</b> <a class="a-tag" href="https://scikit-learn.org/stable/glossary.html#term-sparse-graph" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sparse graph">sparse graph</a> (with, mode='distance'). Graphs are build on indexing techniques like <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html" target="_blank" title="BallTree">BallTree</a> (<b>good for huge dataset</b>) and <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html" target="_blank" title="KDTree">KDTree</a> (<b>good for medium sized dataset</b>).</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer" target="_blank" title="Nearest Neighbors Transformer">Nearest Neighbors Transformer</a> (estimators) rely on <a class="a-tag" href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors" target="_blank" title="nearest neighbors">nearest neighbors</a> classifiers and regressors.<br>Clustering methods: <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" target="_blank" title="DBSCAN">DBSCAN</a> and <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" target="_blank" title="SpectralClustering">SpectralClustering</a> are used for removing noisy features or <a class="a-tag" href="https://en.wikipedia.org/wiki/Anomaly_detection" target="_blank" title="anomaly detection">anomaly detection</a> or outliers detection., <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-regression" target="_blank" title="Nearest Neighbors Regression">Nearest Neighbors Regression</a> for face completion., <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/neighbors.html#neighborhood-components-analysis" target="_blank" title="Neighborhood Components Analysis">Neighborhood Components Analysis</a> (NCA) is a variant of leave-one-out k-nearest neighbors (KNN). NCA can be used to perform supervised <a class="a-tag" href="https://en.wikipedia.org/wiki/Dimensionality_reduction" target="_blank" title="dimensionality reduction">dimensionality reduction</a>, also NCA handles multi-class <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="classification">classification</a> problems naturally with no increase in the model size or need of hypertuning. <a class="a-tag" href="https://scikit-learn.org/stable/glossary.html#term-sparse-graph" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Sparse graph">Sparse graph</a> helps: 1. Low compute time from precomputed graph. 2. Graph caching using <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="pipeline">pipeline</a>.</td>
<td class="table-rows" style="width: 30%;"><b>n_neighbors</b>: number of neighbours; <b>p</b> is <a class="a-tag" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html" target="_blank" title="minkowski metric">minkowski metric</a>, for p = 1 <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html" target="_blank" title="manhattan distance">manhattan distance</a> is used, and uses <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html" target="_blank" title="euclidean distances">euclidean distances</a> for p = 2. <br>Algorithm is set via, <b>algorithm</b>='auto' or 'ball_tree' or 'kd_tree' or 'brute'. <b>auto</b> uses algorithm based fit(); <b>ball_tree</b> uses <a class="a-font" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.BallTree.html" target="_blank" title="BallTree">BallTree</a> (<b>for huge dataset</b>), <b>kd_tree</b> uses <a class="a-font" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KDTree.html" target="_blank" title="KDTree">KDTree</a> (<b>efficient for <20 neighbors</b>); <b>brute</b> will use brute-force search (<b>for small dataset</b>). <b>leaf_size</b> BallTree and KDTree use fast brute force searches within leaf nodes. Low Bias & Low Variance (High -> Overfitting) from sklearn import neighbors; from sklearn.cross_validation import KFold (Mean & Std Deviation).</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-transformer" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="nearest neighbors estimators">Nearest neighbors estimators</a>:  <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.neighbors.KNeighborsClassifier">sklearn.neighbors.KNeighborsClassifier</a>,<br>
<a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.neighbors.KNeighborsRegressor">sklearn.neighbors.KNeighborsRegressor</a>,<br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.cluster.SpectralClustering">sklearn.cluster.SpectralClustering</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.cluster.DBSCAN">sklearn.cluster.DBSCAN</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.manifold.TSNE">sklearn.manifold.TSNE</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html"  target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.manifold.Isomap">sklearn.manifold.Isomap</a>, use <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsTransformer.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.neighbors.KNeighborsTransformer">sklearn.neighbors.KNeighborsTransformer</a>, & <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsTransformer.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.neighbors.RadiusNeighborsTransformer">sklearn.neighbors.RadiusNeighborsTransformer</a>. <a class="a-tag" href="https://scikit-learn.org/stable/moduls/generated/sklearn.neighbors.kneighbors_graph.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.neighbors.kneighbors_graph()">sklearn.neighbors.kneighbors_graph()</a> & <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.radius_neighbors_graph.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.neighbors.radius_neighbors_graph()">sklearn.neighbors.radius_neighbors_graph()</a>. <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.manhattan_distances.html" target="_blank" title="sklearn.metrics.pairwise.manhattan_distances">sklearn.metrics.pairwise.manhattan_distances</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html" target="_blank" title="sklearn.metrics.pairwise.euclidean_distances">sklearn.metrics.pairwise.euclidean_distances</a>, <br><a class="a-tag" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html" target="_blank" title="scipy.spatial.distance.minkowski">scipy.spatial.distance.minkowski</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html" target="_blank" title="sklearn.neighbors.DistanceMetric">sklearn.neighbors.DistanceMetric</a>.</td>
<td class="table-rows" style="width: 5%;"><span style="color: green;">Commonly used for its ease of interpretation and low calculation time.</span></td>


</tr>

<tr><td id="naibay" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;">Prediction of playing outside possible or not based on the temperature dataset. Dataset has these features: <b>overlook, temperature, humidity, wind, and play</b>. We will be predicting whether play will be <b>Yes</b> or <b>No</b></td>
<td class="table-rows" style="width: 10%;"><a class="a-algoname" href="https://scikit-learn.org/stable/modules/naive_bayes.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Naive Bayes">Naive Bayes</a> is powerful algo for prediction using <a class="a-tag" href="https://en.wikipedia.org/wiki/Bayes%27_theorem" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Bayes theorem">Bayes theorem</a> assuming all involved <a class="a-tag" href="https://en.wikipedia.org/wiki/Independent_variable" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="independent feature variables">independent feature variables</a> even if they are dependent in real life.</td>
<td class="table-rows" style="width: 40%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 5%;"><span style="color: green;">Preferred for very large datasets due to simple approach and faster execution.</span><br><span style="color: green;">Has best accuracy of prediction in its class</span></td>


</tr>

<tr><td id="dectre" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;">Prediction of playing outside possible or not based on the temperature dataset. Dataset has these features: <b>overlook, temperature, humidity, wind, and play</b>. We will be predicting whether play will be <b>Yes</b> or <b>No</b></td>
<td class="table-rows" style="width: 10%;"><a class="a-algoname" href="https://scikit-learn.org/stable/modules/tree.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Decision tree">Decision tree</a> is used for decision making or prediction. Binary prediction (Yes/No etc can be had via <a class="a-tag" href="https://scikit-learn.org/stable/modules/tree.html#classification" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="classification">classification</a> and Continuous prediction can be had via <a class="a-tag" href="https://scikit-learn.org/stable/modules/tree.html#regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="regression">regression</a>.</td>
<td class="table-rows" style="width: 40%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/tree.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Decision tree">Decision tree</a> guiding algorithms:<br><a class="a-tag" href="https://en.wikipedia.org/wiki/ID3_algorithm" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="ID3 algorithm">ID3 algorithm</a> says split tree with the feature that helps to get to the leaf node quick. <a class="a-tag" href="https://en.wikipedia.org/wiki/C5.0" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="C5.0">C5.0</a> uses less memory and builds smaller rulesets than <a class="a-tag" href="https://en.wikipedia.org/wiki/C4.5_algorithm" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="C4.5">C4.5</a> while being more accurate, removing categorical features restrictions by <b>ID3</b>. <a class="a-tag" href="https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="CART">CART</a> is quite similar to <b>C4.5</b>. <b>CART</b> does not compute rule sets and uses <a class="a-tag" href="https://en.wikipedia.org/wiki/Binary_tree" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="binary trees">binary trees</a> feature split at each node with <b>InfoGain</b>. Splitting of the tree takes a top down <a class="a-tag" href="https://en.wikipedia.org/wiki/Greedy_algorithm" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="greedy algorithm">greedy algorithm</a> also called <a class="a-tag" href="https://en.wikipedia.org/wiki/Recursive_partitioning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="recursive partitioning">recursive partitioning</a> <a class="a-tag" href="https://en.wikipedia.org/wiki/Binary_splitting" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="binary splitting">binary splitting</a>. Right <b>feature</b> split will get to the leaf node with minimum tree depth thus low <b>uncertainty or randomness</b>. <b>Higher prediction can be had with low uncertainty</b>. Splitting of huge feature into multiple decision trees can also help in getting low uncertainly or low tree depth <b>(majority vote)</b>. Can be used for <a class="a-tag" href="https://scikit-learn.org/stable/auto_examples/plot_multioutput_face_completion.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="facial completion">facial completion</a> via multiple output approach with multiple decision trees predicting single output.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/tree.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Decision tree">Decision tree</a>s use few impurity measures: <a class="a-tag" href="https://en.wikipedia.org/wiki/Entropy_(information_theory)" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Entropy">Entropy</a> <b>(H)</b>, <a class="a-tag" href="https://en.wikipedia.org/wiki/Information_gain" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Information gain">Information gain</a> (decrease in entropy after split on a feature) <b>InfoGain (IG)</b>, and <a class="a-tag" href="https://en.wikipedia.org/wiki/Gini_coefficient" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Gini index">Gini index</a> <b>(GI)</b>. <br><b>Entropy (H)</b> tells if data is random or uncertain, If H is low, uncertainty is low hence prediction is easy. <br>Higher the <b>InfoGain (IG)</b> better it is. We calculate InfoGain for all the columns and choose the one with highest. <br><b>Gini index (GI)</b> can be for <b>binary</b> target values OR <b>categorical</b> target values both. <b>Gini index</b> measures the inequality among features values. Criteria (default=gini) for Gini impurity and Entropy for the information gain.</td>
<td class="table-rows" style="width: 30%;"><b>Entropy</b> (0-1) 0 least impurity, 1 maximum impurity. <br><b>Gini Index</b> (0-1 or 100%). 0 - perfect equality, where all values are the same (where all have same income). one or 100% expresses maximum inequality among values. <br>Feature with maximum <b>InfoGain (IG)</b> forms the root node of the decision tree. <b>min_weight_fraction_leaf</b> min. number of node samples to be considered a leaf, controls the depth and complexity of a tree. <b>max_features</b> feature numners for best node split. <b>criterian</b> measures split quality.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn tree DecisionTreeClassifier">sklearn tree DecisionTreeClassifier</a> (uses default criteria=gini for GI and entropy for IG), <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.tree .DecisionTreeRegressor">sklearn.tree.DecisionTreeRegressor</a> (uses default criteria=mse minimizes L2 loss, friendman_mse, and mae minimizes L1 loss.)</td>
<td class="table-rows" style="width: 5%;"><span style="color: red;">Suffers from <b>low bias</b> (training dataset) and <b>high variance</b> (test dataset) due to <a class="a-tag" href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Overfitting">Overfitting</a></span></td>


</tr>

<tr><td id="ranfor" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;">Prediction of playing outside possible or not based on the temperature dataset. Dataset has these features: <b>overlook, temperature, humidity, wind, and play</b>. We will be predicting whether play will be <b>Yes</b> or <b>No</b></td>
<td class="table-rows" style="width: 10%;"><a class="a-algoname" href="https://en.wikipedia.org/wiki/Random_forest" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Random forests">Random forests</a> are an <a class="a-tag" href="https://scikit-learn.org/stable/modules/ensemble.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="ensemble">ensemble</a> learning method for <a class="a-tag" href="https://scikit-learn.org/stable/modules/tree.html#classification" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="classification">classification</a>, <a class="a-tag" href="https://scikit-learn.org/stable/modules/tree.html#regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="regression">regression</a> and other tasks. Random forests provide correct over decision trees habit of overfitting over their training dataset.</td>
<td class="table-rows" style="width: 40%;"><a class="a-tag" href="https://en.wikipedia.org/wiki/Random_forest" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Random forests">Random forests</a> is based on <a class="a-tag" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Bootstrap aggregating">Bootstrap aggregating</a> or <b>bagging</b>. We prepare binary decision trees (DT1, DT2, DT3, DT4, .. DTc). All the DT(1-c) called bootstrap are trained. Then ALL DT(1-c) are tested on test data providing accuracy or prediction. For <a class="a-tag" href="https://scikit-learn.org/stable/modules/tree.html#classification" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="classification">classification</a> technique aggregating DT(1-c) we will use majority vote. Majority vote will help change <b>high Variance</b> from the original decision tree to <b>low Variance</b>. Similarly for <a class="a-tag" href="https://scikit-learn.org/stable/modules/tree.html#regression" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="regression">regression</a> we take <a class="a-tag" href="https://en.wikipedia.org/wiki/Mean" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="mean">mean</a> or <a class="a-tag" href="https://en.wikipedia.org/wiki/Median" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="median">median</a> of all the predicted values.</td>
<td class="table-rows" style="width: 30%;">Per  <a class="a-tag" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Bootstrap aggregating">Bootstrap aggregating</a> or <b>bagging</b> we create base models (M1, M2, M3, M4, ..Mk) based on binary decision trees (DT1, DT2, DT3, DT4, .. DTc) from base dataset (DS) of R rows and C columns/features. Each decision tree (DTc) can have r rows and c columns/features. C > c, R > r, and k=1..n. DT1 - M1 will have rows and features selected from base dataset (DS) via <a class="a-tag" href="https://en.wikipedia.org/wiki/Sampling_(statistics)" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Row Sampliing and Feature sampling">Row Sampliing and Feature sampling</a> with replacement i.e. in every next decision tree DTc model Mc some records may be repeated from earlier samples and some new.</td>
<td class="table-rows" style="width: 30%;"><b>n_features</b> & <b>n_estimators</b> must be optimum; <b>max_features</b>: random features subset size. If low/high then low/high variance & bias. Default max_features=None (all features) for <b>regression</b> problems,& max_features="sqrt" (random subset of sqrt(n_features)) for <b>classification</b> tasks. <br><b>For good results</b> 1. Set max_depth=None & min_samples_split=2 (hi RAM). 2. Cross-validate parameters. 3. Bootstraping, for out-of-bag <b>oob_score</b>=True. 4. To reduce model size, set: <b>min_samples_split, max_leaf_nodes, max_depth and min_samples_leaf</b>. 5. Parallel computation <b>n_jobs</b>=k (jobs), If n_jobs=-1 all cores used.</td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.ensemble.RandomForestClassifier">sklearn.ensemble.RandomForestClassifier</a>, <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.ensemble.RandomForestRegressor">sklearn.ensemble.RandomForestRegressor</a>, <br>For <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="RandomForestClassifier">RandomForestClassifier</a> and <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="RandomForestRegressor">RandomForestRegressor</a> <a class="a-tag" href="https://en.wikipedia.org/wiki/Bootstrap_aggregating" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Bootstrap aggregating">Bootstrap aggregating</a> aka <b>bagging</b> is by default (bootstrap=True), <br>When (bootstrap=False) full dataset is used  for <a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.ensemble.ExtraTreesClassifier">sklearn.ensemble.ExtraTreesClassifier</a> and <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.ensemble.ExtraTreesRegressor">sklearn.ensemble.ExtraTreesRegressor</a>. <br><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.ensemble.RandomTreesEmbedding">sklearn.ensemble.RandomTreesEmbedding</a><br>Detailed list of <a class="a-tag" href="https://scikit-learn.org/stable/modules/ensemble.html" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="ensemble">ensemble</a> packages are listed <a class="a-tag" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="here">here</a></td>
<td class="table-rows" style="width: 5%;"><span style="color: green;">Helps to overcome <a class="a-tag" href="https://en.wikipedia.org/wiki/Overfitting" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Overfitting">overfitting</a>, and reducing <b>high variance</b> to <b>low variance</b>.</span><br><span style="color: green;">Provides better prediction with accuracy.</span></td>


</tr>

<tr><td id="rfm" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;"></td>
<td class="table-rows" style="width: 10%;">RFM</td>
<td class="table-rows" style="width: 40%;">Helps in finding repeatitive customer behavior. RFM are three dimensions of the model called Response, Frequency and Monetary.</td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 5%;"></td>


</tr>

<tr><td id="aprior" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;"></td>
<td class="table-rows" style="width: 10%;">Apriori</td>
<td class="table-rows" style="width: 40%;">Helps in grouping similar things together. Like similar items, similar buying patterns, similar weather conditions etc</td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 30%;"><a class="a-tag" href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="sklearn.linear_model.LinearRegression">sklearn.linear_model.SGDRegressor</a>"</td>
<td class="table-rows" style="width: 5%;"></td>


</tr>

<tr><td id="adalin" class="table-rows" style="width: 1%; "><a class="a-tag" href="https://en.wikipedia.org/wiki/Supervised_learning" target="_blank" data-lbl="blog" rel="nofollow noopener noreferrer" title="Supervised learning">Supervised learning<a> / <a class="a-tag" href="https://en.wikipedia.org/wiki/Classification" target="_blank" title="Classification">Classification</a></td>
<td class="table-rows" style="width: 15%;"></td>
<td class="table-rows" style="width: 10%;">Adaline</td>
<td class="table-rows" style="width: 40%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 30%;"></td>
<td class="table-rows" style="width: 5%;"></td>


</tr>


</table>



     </div> <!-- END h1-header-container  -->


<!------------------------------------------- PAGE CENTER - END --------------------------------------------->
<!------------------------------------------- PAGE CENTER - END --------------------------------------------->
<!------------------------------------------- PAGE CENTER - END --------------------------------------------->
<!------------------------------------------- PAGE CENTER - END --------------------------------------------->


<!------------------------------------------- PAGE FOOTER --------------------------------------------->
<!------------------------------------------- PAGE FOOTER --------------------------------------------->
<!------------------------------------------- PAGE FOOTER --------------------------------------------->
<!------------------------------------------- PAGE FOOTER --------------------------------------------->

<div id="bottom-footer-id">

     <div class="menu-box-footer-container">
     </div>         <!-- END menu-box-footer-container  -->

          <div class="menu-box-footer-container-two">
          </div>

</div>              <!-- END bottom-footer-id           -->


</div>              <!-- END of org-ddm                 -->


</body>
</html>
